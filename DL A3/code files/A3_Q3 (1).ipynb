{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3bde5f2",
      "metadata": {
        "id": "e3bde5f2"
      },
      "source": [
        "### Implementation 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install datasets\n",
        "#pip install transformers\n",
        "#pip install sacrebleu==1.5.1 datasets portalocker==2.0.0 xxhash==2.0.2\n",
        "#pip install sentencepiece"
      ],
      "metadata": {
        "id": "RI686ZsyyBOC"
      },
      "id": "RI686ZsyyBOC",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "#training dataset\n",
        "train_dataset = load_dataset('wmt16', 'de-en', split='train[:100]')\n",
        "eval_dataset = load_dataset('wmt16', 'de-en', split='test[:10]')\n",
        "\n",
        "max_input_length = 1000\n",
        "max_target_length = 1000\n",
        "source_lang = \"de\"\n",
        "target_lang = \"en\"\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Load the BERT-based encoder-decoder model for machine translation\n",
        "model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-cc25')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = train_dataset.map(preprocess_function, batched=True)\n",
        "encoded_val_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "# # model_name = model_checkpoint.split(\"/\")[-1]\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     \"test-translation\",\n",
        "#     evaluation_strategy = \"epoch\",\n",
        "#     logging_dir='./logs',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     weight_decay=0.01,\n",
        "#     save_total_limit=3,\n",
        "#     num_train_epochs=1,\n",
        "#     predict_with_generate=True,\n",
        "#     fp16=False,\n",
        "# #     push_to_hub=True,\n",
        "# #     push_to_hub_model_id=f\"{model_name}-finetuned-{source_lang}-to-{target-lang}\",\n",
        "# )\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    \"test-translation\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "import nltk\n",
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "# def postprocess_text(preds, labels):\n",
        "#     preds = [\" \".join(pred.strip().split()) for pred in preds]  # Join list of strings\n",
        "#     labels = [[\" \".join(label.strip().split())] for label in labels]  # Join list of strings\n",
        "#     return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "        \n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    # Compute BLEU-1, BLEU-2, and ROUGE-L scores\n",
        "    bleu1 = nltk.translate.bleu_score.corpus_bleu(decoded_labels, decoded_preds, weights=(1, 0))\n",
        "    bleu2 = nltk.translate.bleu_score.corpus_bleu(decoded_labels, decoded_preds, weights=(0.5, 0.5))\n",
        "    \n",
        "    # scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    # scores = scorer.score(decoded_preds, decoded_labels)\n",
        "    # rouge_l = scores['rougeL'].fmeasure\n",
        "    \n",
        "    result = {\"bleu1\": round(bleu1, 4), \"bleu2\": round(bleu2, 4)}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Define the Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    eval_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "PVw4MRY0iruv"
      },
      "id": "PVw4MRY0iruv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Get the training and validation loss and steps from the trainer object\n",
        "# train_loss = trainer.history[\"train_loss\"]\n",
        "# val_loss = trainer.history[\"eval_loss\"]\n",
        "# steps = list(range(len(train_loss)))\n",
        "\n",
        "# # Plot the training and validation loss over the steps\n",
        "# plt.plot(steps, train_loss, label=\"Training loss\")\n",
        "# plt.plot(steps, val_loss, label=\"Validation loss\")\n",
        "\n",
        "# plt.title(\"Training and validation loss\")\n",
        "# plt.xlabel(\"Steps\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qp34f6_YN5rZ"
      },
      "id": "qp34f6_YN5rZ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def postprocess_text(preds, labels):\n",
        "#     preds = [pred.strip() for pred in preds]\n",
        "#     labels = [[label.strip()] for label in labels]\n",
        "#     return preds, labels\n",
        "# def compute_metrics(eval_preds):\n",
        "#     preds, labels = eval_preds\n",
        "#     if isinstance(preds, tuple):\n",
        "#         preds = preds[0]\n",
        "#     print(preds,preds.shape,22222222222)\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "#     print(decoded_preds,len(decoded_preds),111111)\n",
        "#     # Replace -100 in the labels as we can't decode them.\n",
        "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "#     print(decoded_labels,len(decoded_labels),111111)\n",
        "#     # Some simple post-processing\n",
        "#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "#     print(decoded_preds,len(decoded_preds),222222222222)\n",
        "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "#     result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "#     result = {k: round(v, 4) for k, v in result.items()}\n",
        "#     return result\n",
        "\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model,\n",
        "#     training_args,\n",
        "#     train_dataset=tokenized_datasets,\n",
        "#     eval_dataset=tokenized_datasets,\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "lcM2UJflNyHM"
      },
      "id": "lcM2UJflNyHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_IxxzVVd1K3"
      },
      "id": "A_IxxzVVd1K3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf] *",
      "language": "python",
      "name": "conda-env-tf-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
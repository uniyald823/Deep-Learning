# -*- coding: utf-8 -*-
"""DL_A2_Part_II final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ifMTE7A_vs8iWMXy0sQjgEfYE2H1BrGX

# PART II: Convolution Neural Network (CNN) (20 marks)

## Task: Develop a neural network to identify the digits. Perform splitting of Train data in training, validation sets with an 80:20 ratio (random stratified) and use the test data from the test.csv file.

### 1. Visualise 5 random images from 5 different digits. [5 marks]
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np; import random
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

import keras
from keras.datasets import mnist

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.utils import to_categorical, plot_model

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, add

(x_train, y_train),(x_test, y_test) = mnist.load_data()

x_val = x_train[:9000]
y_val = y_train[:9000]

x_train = x_train[9000:]
y_train = y_train[9000:]

y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

image_indices = [1, 10, 124, 235, 535, 999]
for image_index in image_indices:
    print(f'The label of the selected datapoint is: {y_train[image_index]}')
    plt.imshow(x_train[image_index], cmap='Greys')
    plt.show()

indexes = np.random.randint(0, x_train.shape[0], size=25)
images = x_train[indexes]
labels = y_train[indexes]

plt.figure(figsize=(10,10))
for i in range(len(indexes)):
    plt.subplot(5, 5, i + 1)
    image = images[i]
    plt.imshow(image, cmap='gray')
    plt.axis('off')
    
plt.show()
plt.close('all')

"""### 2. Setup1: Create a CNN architecture having:

##### (a) a kernel size of 5×5, followed by kernel of 2x2. Use 10 feature maps for the first convolution layer and 20 for the second.

##### (b). Add pooling after each co nvolution.
##### (c). Add a linear layer with 50 neurons. Finally, add a classification head. Use softmax activation function at classification head.
##### (d). In the above setup you are free to test varying pooling techniques, LR rates, optimisers and activation functions, strides and padding.
"""

#############################  STEUP - 1  #####################################

setup_1_model_1 = Sequential()

setup_1_model_1.add(Conv2D(10, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))
setup_1_model_1.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_1.add(Conv2D(20, kernel_size=(2, 2), activation='relu'))
setup_1_model_1.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_1.add(Flatten())

setup_1_model_1.add(Dense(50, activation='linear'))

setup_1_model_1.add(Dense(10, activation='softmax'))

setup_1_model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history_setup_1_model_1 = setup_1_model_1.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val))

score11 = setup_1_model_1.evaluate(x_test, y_test, verbose=0)

print('Test accuracy:', score11[1])

setup_1_model_2 = Sequential()

setup_1_model_2.add(Conv2D(10, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)))
setup_1_model_2.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_2.add(Conv2D(20, kernel_size=(2, 2), activation='sigmoid'))
setup_1_model_2.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_2.add(Flatten())

setup_1_model_2.add(Dense(50, activation='linear'))

setup_1_model_2.add(Dense(10, activation='softmax'))

setup_1_model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history_setup_1_model_2 = setup_1_model_2.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val))
score12 = setup_1_model_2.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score12[1])

setup_1_model_3 = Sequential()

setup_1_model_3.add(Conv2D(10, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))
setup_1_model_3.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_3.add(Conv2D(20, kernel_size=(2, 2), activation='relu'))
setup_1_model_3.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_3.add(Flatten())

setup_1_model_3.add(Dense(50, activation='linear'))

setup_1_model_3.add(Dense(10, activation='softmax'))

setup_1_model_3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

history_setup_1_model_3 = setup_1_model_3.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val))
score13 = setup_1_model_3.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score13[1])

setup_1_model_4 = Sequential()

setup_1_model_4.add(Conv2D(10, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)))
setup_1_model_4.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_4.add(Conv2D(20, kernel_size=(2, 2), activation='sigmoid'))
setup_1_model_4.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_4.add(Flatten())

setup_1_model_4.add(Dense(50, activation='linear'))

setup_1_model_4.add(Dense(10, activation='softmax'))

setup_1_model_4.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

history_setup_1_model_4 = setup_1_model_4.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val))
score14 = setup_1_model_4.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score14[1])

setup_1_model_5 = Sequential()

setup_1_model_5.add(Conv2D(10, kernel_size=(5, 5), activation='sigmoid', input_shape=(28, 28, 1)))
setup_1_model_5.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_5.add(Conv2D(20, kernel_size=(2, 2), activation='sigmoid'))
setup_1_model_5.add(MaxPooling2D(pool_size=(2, 2)))

setup_1_model_5.add(Flatten())

setup_1_model_5.add(Dense(50, activation='linear'))

setup_1_model_5.add(Dense(10, activation='softmax'))

setup_1_model_5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history_setup_1_model_5 = setup_1_model_5.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val))
score15 = setup_1_model_5.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score15[1])

perf_setup_1 = pd.DataFrame([score11[1], score12[1], score13[1], score14[1], score15[1]],
             ['Model-1', 'Model-2', 'Model-3', 'Model-4', 'Model-5'], columns = ['Test_Acuuracy'])

perf_setup_1.sort_values('Test_Acuuracy', ascending  = False)

"""The best model from setup - 1 is Model-3

### 3. Setup 2: Update setup 1 by adding a residual connection after one of the layers as you deem fit. [2 marks]
"""

#############################  STEUP - 2  #####################################
# Input
input_layer = keras.layers.Input(shape=(28, 28, 1))

# Convolution Layer - 1
conv1 = keras.layers.Conv2D(10, (5, 5), activation='relu')(input_layer)
pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)

# Convolution Layer - 2
conv2 = keras.layers.Conv2D(20, (5, 5), activation='relu')(pool1)

# Residual Layer
residual = keras.layers.Conv2D(20, (1, 1), strides=(3, 3), padding='same')(conv1)

# Residual Connection
add1 = keras.layers.add([conv2, residual])
pool2 = keras.layers.MaxPooling2D((2, 2))(add1)

flatten = keras.layers.Flatten()(pool2)
dense1 = keras.layers.Dense(50, activation='relu')(flatten)
output_layer = keras.layers.Dense(10, activation='softmax')(dense1)
##################################################################

model_setup_2 = keras.Model(inputs=input_layer, outputs=output_layer)

model_setup_2.compile(optimizer='adam',
               loss='categorical_crossentropy',
               metrics=['accuracy'])

history_setup2 = model_setup_2.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val))

score2 = model_setup_2.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score2[1])

"""**Model 4 from setup 1 performs the best**

### 4. Generate the following plots for each setup: [5 marks]
○ Loss plot - Training Loss and Validation Loss V/s Epochs.

○ Accuracy plot - Training Accuracy, Validation Accuracy V/s Epochs

○ Analyze and Explain the plots obtained

#### PLOT FOR BEST MODEL FROM SETUP - 1
"""

# Loss Plot - Setup - 1
plt.figure(figsize=(14, 7))

# Plot the training and validation loss for Setup 1
plt.subplot(1, 2, 1)
plt.plot(history_setup_1_model_1.history['loss'], label='Training Loss')
plt.plot(history_setup_1_model_1.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss for Setup 1')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


# Plot the training and validation accuracy for Setup 2
plt.subplot(1, 2, 2)
plt.plot(history_setup_1_model_1.history['accuracy'], label='Training Accuracy')
plt.plot(history_setup_1_model_1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Setup 1')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

"""#### PLOT FOR SETUP 2"""

# Loss Plot - Setup - 2 
plt.figure(figsize=(14, 7))

# Plot the training and validation loss for Setup 2
plt.subplot(1, 2, 1)
plt.plot(history_setup2.history['loss'], label='Training Loss')
plt.plot(history_setup2.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss for Setup 2')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


# Plot the training and validation accuracy for Setup 2
plt.subplot(1, 2, 2)
plt.plot(history_setup2.history['accuracy'], label='Training Accuracy')
plt.plot(history_setup2.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Setup 2')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()



